{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "46ade2a1807aadd90e577c496d77d3df507dad88",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # to handle matrix and data operation\n",
    "import pandas as pd # to read csv and handle dataframe\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "a87c9de979fb54874e3a047d40cc024a8b0f5e98",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'D:\\dl\\mnist\\train.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "0b3d551a62defaadd37e681511ebc5fc70ac944d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y = df['label'].values\n",
    "X = df.drop(['label'],axis=1).values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "e76dc80a7695aec80f78fa1f62d84bb0ed0efd92",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6300,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "96f0d5dbc90457eb091fb2e6ed68ce7c7bf6da0b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "torch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\n",
    "torch_y_train = torch.from_numpy(y_train).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# create feature and targets tensor for test set.\n",
    "torch_X_test = torch.from_numpy(X_test).type(torch.LongTensor)\n",
    "torch_y_test = torch.from_numpy(y_test).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
    "test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "3624b779d746e6b5710711c7b1798363ecabafbb",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=784, out_features=250, bias=True)\n",
      "  (linear2): Linear(in_features=250, out_features=100, bias=True)\n",
      "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784,250)\n",
    "        self.linear2 = nn.Linear(250,100)\n",
    "        self.linear3 = nn.Linear(100,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    " \n",
    "mlp = MLP()\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c2c74a49c925222b6aaec5e29007a302d2fae44"
   },
   "source": [
    "We have 784\\*(250+1) + 250\\*(100+1) + 100\\*(10+1) = 222 360 parameters to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "e471e447a9618edc7310bb15941054c30ea235a4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fit(model, train_loader):\n",
    "    optimizer = torch.optim.Adam(model.parameters())#,lr=0.001, betas=(0.9,0.999))\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    EPOCHS = 5\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        correct = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Total correct predictions\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            #print(correct)\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data[0], float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (896x28 and 784x250)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, train_loader)\u001b[0m\n\u001b[0;32m     10\u001b[0m var_y_batch \u001b[38;5;241m=\u001b[39m Variable(y_batch)\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_X_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m error(output, var_y_batch)\n\u001b[0;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,X):\n\u001b[1;32m----> 9\u001b[0m     X \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m     X \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(X))\n\u001b[0;32m     11\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear3(X)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (896x28 and 784x250)"
     ]
    }
   ],
   "source": [
    "fit(mlp, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "777eb068065662bea355507597b158eb8713b7f2"
   },
   "source": [
    "## MLP Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a3aca2215610465fcccc58f1b98fc1d56096f364",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "#model = mlp\n",
    "    correct = 0 \n",
    "    for test_imgs, test_labels in test_loader:\n",
    "        #print(test_imgs.shape)\n",
    "        test_imgs = Variable(test_imgs).float()\n",
    "        output = model(test_imgs)\n",
    "        predicted = torch.max(output,1)[1]\n",
    "        correct += (predicted == test_labels).sum()\n",
    "    print(\"Test accuracy:{:.3f}% \".format( float(correct) / (len(test_loader)*BATCH_SIZE)))\n",
    "evaluate(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "5e84347012cd6abc2f6f46299aab08b0c89563ee",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35700, 1, 28, 28])\n",
      "torch.Size([6300, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "torch_X_train = torch_X_train.view(-1, 1,28,28).float()\n",
    "torch_X_test = torch_X_test.view(-1,1,28,28).float()\n",
    "print(torch_X_train.shape)\n",
    "print(torch_X_test.shape)\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
    "test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "c669e5f557724154d9f4f5f8b5d0d6c9b676d551",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(32,64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(3*3*64, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x),2))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.view(-1,3*3*64 )\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    " \n",
    "cnn = CNN()\n",
    "print(cnn)\n",
    "\n",
    "it = iter(train_loader)\n",
    "X_batch, y_batch = next(it)\n",
    "print(cnn.forward(X_batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "797cbe3cff05d8bdb42f3273a06839b18daf5266",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 23\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, train_loader)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#print(correct)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m)]\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Accuracy:\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m---> 23\u001b[0m         epoch, batch_idx\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(X_batch), \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset), \u001b[38;5;241m100.\u001b[39m\u001b[38;5;241m*\u001b[39mbatch_idx \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader), \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mfloat\u001b[39m(correct\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(BATCH_SIZE\u001b[38;5;241m*\u001b[39m(batch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))))\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "fit(cnn,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "890dfc8d29ac70df919807b275b38112ca9297ab",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "evaluate(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "3703af54a320fd81bfa426be17655a3cebec5ac4",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 [0/35700 (0%)]\tLoss: 0.037949\t Accuracy:100.000%\n",
      "Epoch : 1 [1600/35700 (4%)]\tLoss: 0.299476\t Accuracy:97.243%\n",
      "Epoch : 1 [3200/35700 (9%)]\tLoss: 0.034291\t Accuracy:96.627%\n",
      "Epoch : 1 [4800/35700 (13%)]\tLoss: 0.443208\t Accuracy:96.834%\n",
      "Epoch : 1 [6400/35700 (18%)]\tLoss: 0.085162\t Accuracy:96.828%\n",
      "Epoch : 1 [8000/35700 (22%)]\tLoss: 0.389957\t Accuracy:96.813%\n",
      "Epoch : 1 [9600/35700 (27%)]\tLoss: 0.210955\t Accuracy:96.854%\n",
      "Epoch : 1 [11200/35700 (31%)]\tLoss: 0.491646\t Accuracy:96.813%\n",
      "Epoch : 1 [12800/35700 (36%)]\tLoss: 0.050255\t Accuracy:96.774%\n",
      "Epoch : 1 [14400/35700 (40%)]\tLoss: 0.158110\t Accuracy:96.619%\n",
      "Epoch : 1 [16000/35700 (45%)]\tLoss: 0.121028\t Accuracy:96.650%\n",
      "Epoch : 1 [17600/35700 (49%)]\tLoss: 0.369738\t Accuracy:96.591%\n",
      "Epoch : 1 [19200/35700 (54%)]\tLoss: 0.000867\t Accuracy:96.589%\n",
      "Epoch : 1 [20800/35700 (58%)]\tLoss: 0.009598\t Accuracy:96.573%\n",
      "Epoch : 1 [22400/35700 (63%)]\tLoss: 0.116083\t Accuracy:96.567%\n",
      "Epoch : 1 [24000/35700 (67%)]\tLoss: 0.071074\t Accuracy:96.542%\n",
      "Epoch : 1 [25600/35700 (72%)]\tLoss: 0.210352\t Accuracy:96.543%\n",
      "Epoch : 1 [27200/35700 (76%)]\tLoss: 0.048688\t Accuracy:96.541%\n",
      "Epoch : 1 [28800/35700 (81%)]\tLoss: 0.068192\t Accuracy:96.573%\n",
      "Epoch : 1 [30400/35700 (85%)]\tLoss: 0.004450\t Accuracy:96.546%\n",
      "Epoch : 1 [32000/35700 (90%)]\tLoss: 0.041014\t Accuracy:96.560%\n",
      "Epoch : 1 [33600/35700 (94%)]\tLoss: 0.016229\t Accuracy:96.593%\n",
      "Epoch : 1 [35200/35700 (99%)]\tLoss: 0.037440\t Accuracy:96.631%\n",
      "Epoch : 2 [0/35700 (0%)]\tLoss: 0.025853\t Accuracy:100.000%\n",
      "Epoch : 2 [1600/35700 (4%)]\tLoss: 0.231377\t Accuracy:95.650%\n",
      "Epoch : 2 [3200/35700 (9%)]\tLoss: 0.047404\t Accuracy:95.545%\n",
      "Epoch : 2 [4800/35700 (13%)]\tLoss: 0.073589\t Accuracy:96.192%\n",
      "Epoch : 2 [6400/35700 (18%)]\tLoss: 0.169885\t Accuracy:96.300%\n",
      "Epoch : 2 [8000/35700 (22%)]\tLoss: 0.142738\t Accuracy:96.377%\n",
      "Epoch : 2 [9600/35700 (27%)]\tLoss: 0.026396\t Accuracy:96.439%\n",
      "Epoch : 2 [11200/35700 (31%)]\tLoss: 0.001797\t Accuracy:96.528%\n",
      "Epoch : 2 [12800/35700 (36%)]\tLoss: 0.037655\t Accuracy:96.540%\n",
      "Epoch : 2 [14400/35700 (40%)]\tLoss: 0.319106\t Accuracy:96.522%\n",
      "Epoch : 2 [16000/35700 (45%)]\tLoss: 0.183029\t Accuracy:96.495%\n",
      "Epoch : 2 [17600/35700 (49%)]\tLoss: 0.173489\t Accuracy:96.501%\n",
      "Epoch : 2 [19200/35700 (54%)]\tLoss: 0.007218\t Accuracy:96.511%\n",
      "Epoch : 2 [20800/35700 (58%)]\tLoss: 0.017523\t Accuracy:96.429%\n",
      "Epoch : 2 [22400/35700 (63%)]\tLoss: 0.016448\t Accuracy:96.425%\n",
      "Epoch : 2 [24000/35700 (67%)]\tLoss: 0.082768\t Accuracy:96.388%\n",
      "Epoch : 2 [25600/35700 (72%)]\tLoss: 0.178483\t Accuracy:96.376%\n",
      "Epoch : 2 [27200/35700 (76%)]\tLoss: 0.002847\t Accuracy:96.398%\n",
      "Epoch : 2 [28800/35700 (81%)]\tLoss: 0.010891\t Accuracy:96.414%\n",
      "Epoch : 2 [30400/35700 (85%)]\tLoss: 0.108542\t Accuracy:96.402%\n",
      "Epoch : 2 [32000/35700 (90%)]\tLoss: 0.047979\t Accuracy:96.391%\n",
      "Epoch : 2 [33600/35700 (94%)]\tLoss: 0.017941\t Accuracy:96.417%\n",
      "Epoch : 2 [35200/35700 (99%)]\tLoss: 0.011763\t Accuracy:96.421%\n",
      "Epoch : 3 [0/35700 (0%)]\tLoss: 0.008494\t Accuracy:100.000%\n",
      "Epoch : 3 [1600/35700 (4%)]\tLoss: 0.147113\t Accuracy:96.262%\n",
      "Epoch : 3 [3200/35700 (9%)]\tLoss: 0.029216\t Accuracy:96.101%\n",
      "Epoch : 3 [4800/35700 (13%)]\tLoss: 0.330172\t Accuracy:96.213%\n",
      "Epoch : 3 [6400/35700 (18%)]\tLoss: 0.362827\t Accuracy:96.051%\n",
      "Epoch : 3 [8000/35700 (22%)]\tLoss: 0.192547\t Accuracy:96.228%\n",
      "Epoch : 3 [9600/35700 (27%)]\tLoss: 0.140119\t Accuracy:96.335%\n",
      "Epoch : 3 [11200/35700 (31%)]\tLoss: 0.006674\t Accuracy:96.412%\n",
      "Epoch : 3 [12800/35700 (36%)]\tLoss: 0.002107\t Accuracy:96.548%\n",
      "Epoch : 3 [14400/35700 (40%)]\tLoss: 0.300766\t Accuracy:96.529%\n",
      "Epoch : 3 [16000/35700 (45%)]\tLoss: 0.240378\t Accuracy:96.557%\n",
      "Epoch : 3 [17600/35700 (49%)]\tLoss: 0.163366\t Accuracy:96.529%\n",
      "Epoch : 3 [19200/35700 (54%)]\tLoss: 0.012099\t Accuracy:96.511%\n",
      "Epoch : 3 [20800/35700 (58%)]\tLoss: 0.062971\t Accuracy:96.501%\n",
      "Epoch : 3 [22400/35700 (63%)]\tLoss: 0.015388\t Accuracy:96.447%\n",
      "Epoch : 3 [24000/35700 (67%)]\tLoss: 0.126040\t Accuracy:96.446%\n",
      "Epoch : 3 [25600/35700 (72%)]\tLoss: 0.147902\t Accuracy:96.426%\n",
      "Epoch : 3 [27200/35700 (76%)]\tLoss: 0.064242\t Accuracy:96.423%\n",
      "Epoch : 3 [28800/35700 (81%)]\tLoss: 0.361576\t Accuracy:96.441%\n",
      "Epoch : 3 [30400/35700 (85%)]\tLoss: 0.028844\t Accuracy:96.425%\n",
      "Epoch : 3 [32000/35700 (90%)]\tLoss: 0.067057\t Accuracy:96.454%\n",
      "Epoch : 3 [33600/35700 (94%)]\tLoss: 0.074302\t Accuracy:96.465%\n",
      "Epoch : 3 [35200/35700 (99%)]\tLoss: 0.004933\t Accuracy:96.461%\n",
      "Epoch : 4 [0/35700 (0%)]\tLoss: 0.018847\t Accuracy:100.000%\n",
      "Epoch : 4 [1600/35700 (4%)]\tLoss: 0.362589\t Accuracy:96.691%\n",
      "Epoch : 4 [3200/35700 (9%)]\tLoss: 0.057072\t Accuracy:96.256%\n",
      "Epoch : 4 [4800/35700 (13%)]\tLoss: 0.036676\t Accuracy:96.502%\n",
      "Epoch : 4 [6400/35700 (18%)]\tLoss: 0.084896\t Accuracy:96.564%\n",
      "Epoch : 4 [8000/35700 (22%)]\tLoss: 0.452811\t Accuracy:96.626%\n",
      "Epoch : 4 [9600/35700 (27%)]\tLoss: 0.157749\t Accuracy:96.564%\n",
      "Epoch : 4 [11200/35700 (31%)]\tLoss: 0.019731\t Accuracy:96.537%\n",
      "Epoch : 4 [12800/35700 (36%)]\tLoss: 0.169816\t Accuracy:96.633%\n",
      "Epoch : 4 [14400/35700 (40%)]\tLoss: 0.154717\t Accuracy:96.487%\n",
      "Epoch : 4 [16000/35700 (45%)]\tLoss: 0.126950\t Accuracy:96.476%\n",
      "Epoch : 4 [17600/35700 (49%)]\tLoss: 0.240305\t Accuracy:96.506%\n",
      "Epoch : 4 [19200/35700 (54%)]\tLoss: 0.260561\t Accuracy:96.449%\n",
      "Epoch : 4 [20800/35700 (58%)]\tLoss: 0.087189\t Accuracy:96.395%\n",
      "Epoch : 4 [22400/35700 (63%)]\tLoss: 0.101865\t Accuracy:96.389%\n",
      "Epoch : 4 [24000/35700 (67%)]\tLoss: 0.132646\t Accuracy:96.322%\n",
      "Epoch : 4 [25600/35700 (72%)]\tLoss: 0.187266\t Accuracy:96.368%\n",
      "Epoch : 4 [27200/35700 (76%)]\tLoss: 0.038529\t Accuracy:96.416%\n",
      "Epoch : 4 [28800/35700 (81%)]\tLoss: 0.106233\t Accuracy:96.473%\n",
      "Epoch : 4 [30400/35700 (85%)]\tLoss: 0.008676\t Accuracy:96.527%\n",
      "Epoch : 4 [32000/35700 (90%)]\tLoss: 0.056904\t Accuracy:96.522%\n",
      "Epoch : 4 [33600/35700 (94%)]\tLoss: 0.118069\t Accuracy:96.536%\n",
      "Epoch : 4 [35200/35700 (99%)]\tLoss: 0.095390\t Accuracy:96.526%\n",
      "Epoch : 5 [0/35700 (0%)]\tLoss: 0.026706\t Accuracy:100.000%\n",
      "Epoch : 5 [1600/35700 (4%)]\tLoss: 0.178597\t Accuracy:96.814%\n",
      "Epoch : 5 [3200/35700 (9%)]\tLoss: 0.004223\t Accuracy:96.597%\n",
      "Epoch : 5 [4800/35700 (13%)]\tLoss: 0.164680\t Accuracy:96.772%\n",
      "Epoch : 5 [6400/35700 (18%)]\tLoss: 0.103215\t Accuracy:96.828%\n",
      "Epoch : 5 [8000/35700 (22%)]\tLoss: 0.631501\t Accuracy:96.975%\n",
      "Epoch : 5 [9600/35700 (27%)]\tLoss: 0.132007\t Accuracy:96.948%\n",
      "Epoch : 5 [11200/35700 (31%)]\tLoss: 0.019494\t Accuracy:96.813%\n",
      "Epoch : 5 [12800/35700 (36%)]\tLoss: 0.090175\t Accuracy:96.789%\n",
      "Epoch : 5 [14400/35700 (40%)]\tLoss: 0.174650\t Accuracy:96.764%\n",
      "Epoch : 5 [16000/35700 (45%)]\tLoss: 0.111171\t Accuracy:96.756%\n",
      "Epoch : 5 [17600/35700 (49%)]\tLoss: 0.152420\t Accuracy:96.756%\n",
      "Epoch : 5 [19200/35700 (54%)]\tLoss: 0.047916\t Accuracy:96.719%\n",
      "Epoch : 5 [20800/35700 (58%)]\tLoss: 0.010929\t Accuracy:96.707%\n",
      "Epoch : 5 [22400/35700 (63%)]\tLoss: 0.293233\t Accuracy:96.723%\n",
      "Epoch : 5 [24000/35700 (67%)]\tLoss: 0.227594\t Accuracy:96.704%\n",
      "Epoch : 5 [25600/35700 (72%)]\tLoss: 0.440585\t Accuracy:96.699%\n",
      "Epoch : 5 [27200/35700 (76%)]\tLoss: 0.180768\t Accuracy:96.688%\n",
      "Epoch : 5 [28800/35700 (81%)]\tLoss: 0.277042\t Accuracy:96.702%\n",
      "Epoch : 5 [30400/35700 (85%)]\tLoss: 0.009388\t Accuracy:96.737%\n",
      "Epoch : 5 [32000/35700 (90%)]\tLoss: 0.077504\t Accuracy:96.747%\n",
      "Epoch : 5 [33600/35700 (94%)]\tLoss: 0.024480\t Accuracy:96.738%\n",
      "Epoch : 5 [35200/35700 (99%)]\tLoss: 0.011286\t Accuracy:96.727%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 64\u001b[0m\n\u001b[0;32m     60\u001b[0m fit(cnn, train_loader)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# fit(mlp, train_loader)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[43mevaluate\u001b[49m(cnn)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784, 250)\n",
    "        self.linear2 = nn.Linear(250, 100)\n",
    "        self.linear3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the input\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 256)  # Adjusted input features\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 64 * 3 * 3)  # Flatten the input\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def fit(model, train_loader):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, 6):\n",
    "        correct = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            var_X_batch = Variable(X_batch)\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(var_y_batch.data.view_as(pred)).sum()\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx * len(X_batch), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item(),\n",
    "                    float(correct * 100) / float(BATCH_SIZE * (batch_idx + 1))))\n",
    "\n",
    "# Example usage:\n",
    "fit(cnn, train_loader)\n",
    "# fit(mlp, train_loader)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluate(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direction vector 1:\n",
      "torch.Size([32, 1, 5, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 32, 5, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([64, 32, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 576])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n",
      "Direction vector 2:\n",
      "torch.Size([32, 1, 5, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 32, 5, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([64, 32, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 576])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n",
      "Direction vector 3:\n",
      "torch.Size([32, 1, 5, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 32, 5, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([64, 32, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 576])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n",
      "Direction vector 4:\n",
      "torch.Size([32, 1, 5, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 32, 5, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([64, 32, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 576])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n",
      "Direction vector 5:\n",
      "torch.Size([32, 1, 5, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 32, 5, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([64, 32, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 576])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "def generate_random_direction_vectors(model, num_vectors=10):\n",
    "    direction_vectors = []\n",
    "    for _ in range(num_vectors):\n",
    "        direction_vector = []\n",
    "        for param in model.parameters():\n",
    "            direction_vector.append(torch.randn_like(param))\n",
    "        direction_vectors.append(direction_vector)\n",
    "    return direction_vectors\n",
    "\n",
    "random_directions = generate_random_direction_vectors(cnn, num_vectors=5)\n",
    "for i, direction in enumerate(random_directions):\n",
    "    print(f\"Direction vector {i+1}:\")\n",
    "    for tensor in direction:\n",
    "        print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_model(model,direction1,direction2,alpha,beta):\n",
    "    new_model = CNN()\n",
    "    for new_param, param, d1, d2 in zip(new_model.parameters(), model.parameters(), direction1, direction2):\n",
    "        new_param.data = param.data + alpha * d1 + beta * d2\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 576]' is invalid for input of size 524288",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, beta \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(beta_range):\n\u001b[0;32m     15\u001b[0m         perturbed_model \u001b[38;5;241m=\u001b[39m perturb_model(cnn, random_directions[\u001b[38;5;241m0\u001b[39m], random_directions[\u001b[38;5;241m1\u001b[39m], alpha, beta)\n\u001b[1;32m---> 16\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperturbed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m         loss_landscape[i, j] \u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Plot the 3D loss landscape\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[33], line 10\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(model, data_loader)\u001b[0m\n\u001b[0;32m      8\u001b[0m var_X_batch \u001b[38;5;241m=\u001b[39m Variable(X_batch)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m      9\u001b[0m var_y_batch \u001b[38;5;241m=\u001b[39m Variable(y_batch)\n\u001b[1;32m---> 10\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_X_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m error(output, var_y_batch)\n\u001b[0;32m     12\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[29], line 33\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))\n\u001b[1;32m---> 33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Flatten the input\u001b[39;00m\n\u001b[0;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 576]' is invalid for input of size 524288"
     ]
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the grid for alpha and beta\n",
    "alpha_range = np.linspace(-1, 1, 20)\n",
    "beta_range = np.linspace(-1, 1, 20)\n",
    "\n",
    "# Initialize the loss landscape\n",
    "loss_landscape = np.zeros((len(alpha_range), len(beta_range)))\n",
    "\n",
    "# Compute the loss for each point in the grid\n",
    "for i, alpha in enumerate(alpha_range):\n",
    "    for j, beta in enumerate(beta_range):\n",
    "        perturbed_model = perturb_model(cnn, random_directions[0], random_directions[1], alpha, beta)\n",
    "        loss = compute_loss(perturbed_model, test_loader)\n",
    "        loss_landscape[i, j] = loss\n",
    "\n",
    "# Plot the 3D loss landscape\n",
    "alpha_grid, beta_grid = np.meshgrid(alpha_range, beta_range)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(alpha_grid, beta_grid, loss_landscape, cmap='viridis')\n",
    "\n",
    "ax.set_xlabel('Alpha')\n",
    "ax.set_ylabel('Beta')\n",
    "ax.set_zlabel('Loss')\n",
    "ax.set_title('3D Loss Landscape')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0016, Accuracy: 6196/6300 (98.35%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# def evaluate(model, test_loader):\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     error = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     with torch.no_grad():  # Disable gradient computation\n",
    "#         for X_batch, y_batch in test_loader:\n",
    "#             var_X_batch = Variable(X_batch)\n",
    "#             var_y_batch = Variable(y_batch)\n",
    "#             output = model(var_X_batch)\n",
    "#             test_loss += error(output, var_y_batch).item()  # Sum up batch loss\n",
    "#             pred = output.data.max(1, keepdim=True)[1]  # Get the index of the max log-probability\n",
    "#             correct += pred.eq(var_y_batch.data.view_as(pred)).sum().item()\n",
    "    \n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "#     accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "#     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "#         test_loss, correct, len(test_loader.dataset), accuracy))\n",
    "\n",
    "# # Example usage:\n",
    "# evaluate(cnn, test_loader)\n",
    "# # evaluate(mlp, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
